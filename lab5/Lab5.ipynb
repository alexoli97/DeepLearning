{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfcBXMCmqQH4",
        "outputId": "456c5c47-d6c2-43ab-c139-25f1bd5d6b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import os\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zdn7VsWPqX0N"
      },
      "outputs": [],
      "source": [
        "def xavier_init(size):\n",
        "    in_dim = size[0]\n",
        "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
        "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-nLejHskqZXT"
      },
      "outputs": [],
      "source": [
        "def plot(samples):\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    gs = gridspec.GridSpec(4, 4)\n",
        "    gs.update(wspace=0.05, hspace=0.05)\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        ax = plt.subplot(gs[i])\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_aspect('equal')\n",
        "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n",
        "def sample_Z(m, n):\n",
        "    return np.random.uniform(-1., 1., size=[m, n])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gty-ar_kqaYb"
      },
      "outputs": [],
      "source": [
        "def generator(z):\n",
        "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
        "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
        "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
        "\n",
        "    return G_prob\n",
        "\n",
        "\n",
        "def discriminator(x):\n",
        "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
        "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
        "    D_prob = tf.nn.sigmoid(D_logit)\n",
        "\n",
        "    return D_prob, D_logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d2CARiygqb-T"
      },
      "outputs": [],
      "source": [
        "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "\n",
        "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
        "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
        "\n",
        "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
        "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
        "\n",
        "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
        "\n",
        "\n",
        "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
        "\n",
        "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
        "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
        "\n",
        "G_W2 = tf.Variable(xavier_init([128, 784]))\n",
        "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
        "\n",
        "theta_G = [G_W1, G_W2, G_b1, G_b2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HdOWTKu5qdFW"
      },
      "outputs": [],
      "source": [
        "G_sample = generator(Z)\n",
        "D_real, D_logit_real = discriminator(X)\n",
        "D_fake, D_logit_fake = discriminator(G_sample)\n",
        "\n",
        "# # Original losses:\n",
        "# # -------------------\n",
        "D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
        "G_loss = -tf.reduce_mean(tf.log(D_fake))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XlKDhzPtBgQ",
        "outputId": "0e9d2545-75bc-4226-8c33-fe0b1bb96af7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qsKN44-MuNlM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class MNISTDataset:\n",
        "    \"\"\"'Bare minimum' class to wrap MNIST numpy arrays into a dataset.\"\"\"\n",
        "    def __init__(self, train_imgs, train_lbs, test_imgs, test_lbls, batch_size,\n",
        "                 to01=True, shuffle=True, seed=None):\n",
        "        \"\"\"\n",
        "        Use seed optionally to always get the same shuffling (-> reproducible\n",
        "        results).\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.train_data = train_imgs\n",
        "        self.train_labels = train_lbs.astype(np.int32)\n",
        "        self.test_data = test_imgs\n",
        "        self.test_labels = test_lbls.astype(np.int32)\n",
        "\n",
        "        if to01:\n",
        "            # int in [0, 255] -> float in [0, 1]\n",
        "            self.train_data = self.train_data.astype(np.float32) / 255\n",
        "            self.test_data = self.test_data.astype(np.float32) / 255\n",
        "\n",
        "        self.size = self.train_data.shape[0]\n",
        "\n",
        "        if seed:\n",
        "            np.random.seed(seed)\n",
        "        if shuffle:\n",
        "            self.shuffle_train()\n",
        "        self.shuffle = shuffle\n",
        "        self.current_pos = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        \"\"\"Either gets the next batch, or optionally shuffles and starts a\n",
        "        new epoch.\"\"\"\n",
        "        end_pos = self.current_pos + self.batch_size\n",
        "        if end_pos < self.size:\n",
        "            batch = (self.train_data[self.current_pos:end_pos],\n",
        "                     self.train_labels[self.current_pos:end_pos])\n",
        "            self.current_pos += self.batch_size\n",
        "        else:\n",
        "            # we return what's left (-> possibly smaller batch!) and prepare\n",
        "            # the start of a new epoch\n",
        "            # batch = (self.train_data[self.current_pos:self.size],\n",
        "            #          self.train_labels[self.current_pos:self.size])\n",
        "            if self.shuffle:\n",
        "                self.shuffle_train()\n",
        "            self.current_pos = 0\n",
        "            end_pos = self.current_pos + self.batch_size\n",
        "            batch = (self.train_data[self.current_pos:end_pos],\n",
        "                     self.train_labels[self.current_pos:end_pos])\n",
        "            self.current_pos += self.batch_size\n",
        "            # print(\"Starting new epoch...\")\n",
        "        return batch\n",
        "\n",
        "    def shuffle_train(self):\n",
        "        shuffled_inds = np.arange(self.train_data.shape[0])\n",
        "        np.random.shuffle(shuffled_inds)\n",
        "        self.train_data = self.train_data[shuffled_inds]\n",
        "        self.train_labels = self.train_labels[shuffled_inds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_lL6Yyfcqepp"
      },
      "outputs": [],
      "source": [
        "# Alternative losses:\n",
        "# # -------------------\n",
        "# D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
        "# D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
        "# D_loss = D_loss_real + D_loss_fake\n",
        "# G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "Fo7tdm_hqfmi",
        "outputId": "900860d5-f580-4336-a7e5-13262685a919"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "plt.imshow(train_images[0], cmap=\"Greys_r\")\n",
        "\n",
        "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
        "                    test_images.reshape([-1, 784]), test_labels,\n",
        "                    batch_size=128)\n",
        "\n",
        "                    \n",
        "for it in range(5000):\n",
        "    X_mb,_ = data.next_batch()\n",
        "    if X_mb.shape == (96,784):\n",
        "        print(X_mb.shape)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xfM-Na3vqh6w"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
        "    G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
        "\n",
        "    mb_size = 128\n",
        "    Z_dim = 100\n",
        "    n_iterations = 100000\n",
        "    path = \"out_org_100k/\"\n",
        "\n",
        "    sess = tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for it in range(n_iterations+1):\n",
        "        if it % 1000 == 0:\n",
        "            samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
        "\n",
        "            fig = plot(samples)\n",
        "            plt.savefig('out_org_100k/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
        "            i += 1\n",
        "            plt.close(fig)\n",
        "\n",
        "        X_mb, _ = data.next_batch()\n",
        "\n",
        "\n",
        "        _, D_loss_curr = sess.run([D_solver, D_loss], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
        "        _, G_loss_curr = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
        "\n",
        "        if math.isnan(D_loss_curr):\n",
        "            print(\"oh no loss became nan at iteration\",it) \n",
        "            return False\n",
        "\n",
        "        if it % 1000 == 0:\n",
        "            print('Iter: {}'.format(it))\n",
        "            print('D loss: {:.4}'. format(D_loss_curr))\n",
        "            print('G_loss: {:.4}'.format(G_loss_curr))\n",
        "            print()\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI3pmnads12D",
        "outputId": "50008932-4a64-4905-9093-f7263e439255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0\n",
            "D loss: 1.417\n",
            "G_loss: 2.559\n",
            "\n",
            "Iter: 1000\n",
            "D loss: 0.003699\n",
            "G_loss: 9.234\n",
            "\n",
            "Iter: 2000\n",
            "D loss: 0.004684\n",
            "G_loss: 7.188\n",
            "\n",
            "Iter: 3000\n",
            "D loss: 0.02898\n",
            "G_loss: 6.092\n",
            "\n",
            "Iter: 4000\n",
            "D loss: 0.09164\n",
            "G_loss: 5.716\n",
            "\n",
            "oh no loss became nan at iteration 4559\n",
            "Iter: 0\n",
            "D loss: 1.172\n",
            "G_loss: 2.645\n",
            "\n",
            "Iter: 1000\n",
            "D loss: 0.01073\n",
            "G_loss: 10.88\n",
            "\n",
            "Iter: 2000\n",
            "D loss: 0.01719\n",
            "G_loss: 6.422\n",
            "\n",
            "Iter: 3000\n",
            "D loss: 0.06238\n",
            "G_loss: 5.661\n",
            "\n",
            "Iter: 4000\n",
            "D loss: 0.1468\n",
            "G_loss: 4.752\n",
            "\n",
            "oh no loss became nan at iteration 4692\n",
            "Iter: 0\n",
            "D loss: 1.352\n",
            "G_loss: 2.32\n",
            "\n",
            "Iter: 1000\n",
            "D loss: 0.008014\n",
            "G_loss: 9.562\n",
            "\n",
            "Iter: 2000\n",
            "D loss: 0.03829\n",
            "G_loss: 5.237\n",
            "\n",
            "Iter: 3000\n",
            "D loss: 0.06799\n",
            "G_loss: 5.364\n",
            "\n",
            "Iter: 4000\n",
            "D loss: 0.2036\n",
            "G_loss: 5.958\n",
            "\n",
            "oh no loss became nan at iteration 4429\n",
            "Iter: 0\n",
            "D loss: 1.968\n",
            "G_loss: 1.857\n",
            "\n",
            "Iter: 1000\n",
            "D loss: 0.01186\n",
            "G_loss: 11.05\n",
            "\n",
            "Iter: 2000\n",
            "D loss: 0.06403\n",
            "G_loss: 5.27\n",
            "\n",
            "Iter: 3000\n",
            "D loss: 0.04806\n",
            "G_loss: 6.207\n",
            "\n",
            "Iter: 4000\n",
            "D loss: 0.08651\n",
            "G_loss: 5.604\n",
            "\n",
            "Iter: 5000\n",
            "D loss: 0.1108\n",
            "G_loss: 4.852\n",
            "\n",
            "Iter: 6000\n",
            "D loss: 0.2606\n",
            "G_loss: 4.806\n",
            "\n",
            "Iter: 7000\n",
            "D loss: 0.3035\n",
            "G_loss: 4.434\n",
            "\n",
            "Iter: 8000\n",
            "D loss: 0.3317\n",
            "G_loss: 4.592\n",
            "\n",
            "Iter: 9000\n",
            "D loss: 0.4202\n",
            "G_loss: 3.261\n",
            "\n",
            "Iter: 10000\n",
            "D loss: 0.5225\n",
            "G_loss: 3.136\n",
            "\n",
            "Iter: 11000\n",
            "D loss: 0.5231\n",
            "G_loss: 2.613\n",
            "\n",
            "Iter: 12000\n",
            "D loss: 0.6926\n",
            "G_loss: 2.87\n",
            "\n",
            "Iter: 13000\n",
            "D loss: 0.3437\n",
            "G_loss: 3.664\n",
            "\n",
            "Iter: 14000\n",
            "D loss: 0.7258\n",
            "G_loss: 2.713\n",
            "\n",
            "Iter: 15000\n",
            "D loss: 0.4548\n",
            "G_loss: 3.183\n",
            "\n",
            "Iter: 16000\n",
            "D loss: 0.589\n",
            "G_loss: 2.431\n",
            "\n",
            "Iter: 17000\n",
            "D loss: 0.4593\n",
            "G_loss: 2.374\n",
            "\n",
            "Iter: 18000\n",
            "D loss: 0.6724\n",
            "G_loss: 2.54\n",
            "\n",
            "Iter: 19000\n",
            "D loss: 0.6987\n",
            "G_loss: 2.458\n",
            "\n",
            "Iter: 20000\n",
            "D loss: 0.5994\n",
            "G_loss: 2.876\n",
            "\n",
            "Iter: 21000\n",
            "D loss: 0.4947\n",
            "G_loss: 3.052\n",
            "\n",
            "Iter: 22000\n",
            "D loss: 0.4832\n",
            "G_loss: 3.15\n",
            "\n",
            "Iter: 23000\n",
            "D loss: 0.5982\n",
            "G_loss: 2.589\n",
            "\n",
            "Iter: 24000\n",
            "D loss: 0.6181\n",
            "G_loss: 2.757\n",
            "\n",
            "Iter: 25000\n",
            "D loss: 0.6997\n",
            "G_loss: 2.566\n",
            "\n",
            "Iter: 26000\n",
            "D loss: 0.6093\n",
            "G_loss: 2.612\n",
            "\n",
            "Iter: 27000\n",
            "D loss: 0.6236\n",
            "G_loss: 2.722\n",
            "\n",
            "Iter: 28000\n",
            "D loss: 0.5874\n",
            "G_loss: 2.869\n",
            "\n",
            "Iter: 29000\n",
            "D loss: 0.6034\n",
            "G_loss: 2.635\n",
            "\n",
            "Iter: 30000\n",
            "D loss: 0.6071\n",
            "G_loss: 3.147\n",
            "\n",
            "Iter: 31000\n",
            "D loss: 0.4923\n",
            "G_loss: 2.523\n",
            "\n",
            "Iter: 32000\n",
            "D loss: 0.5842\n",
            "G_loss: 2.582\n",
            "\n",
            "Iter: 33000\n",
            "D loss: 0.4648\n",
            "G_loss: 3.001\n",
            "\n",
            "Iter: 34000\n",
            "D loss: 0.6106\n",
            "G_loss: 3.022\n",
            "\n",
            "Iter: 35000\n",
            "D loss: 0.484\n",
            "G_loss: 2.46\n",
            "\n",
            "Iter: 36000\n",
            "D loss: 0.6398\n",
            "G_loss: 2.73\n",
            "\n",
            "Iter: 37000\n",
            "D loss: 0.6011\n",
            "G_loss: 2.89\n",
            "\n",
            "Iter: 38000\n",
            "D loss: 0.5916\n",
            "G_loss: 2.536\n",
            "\n",
            "Iter: 39000\n",
            "D loss: 0.6673\n",
            "G_loss: 2.546\n",
            "\n",
            "Iter: 40000\n",
            "D loss: 0.4709\n",
            "G_loss: 2.722\n",
            "\n",
            "Iter: 41000\n",
            "D loss: 0.6291\n",
            "G_loss: 2.476\n",
            "\n",
            "Iter: 42000\n",
            "D loss: 0.5833\n",
            "G_loss: 2.625\n",
            "\n",
            "Iter: 43000\n",
            "D loss: 0.5997\n",
            "G_loss: 2.623\n",
            "\n",
            "Iter: 44000\n",
            "D loss: 0.7041\n",
            "G_loss: 2.55\n",
            "\n",
            "Iter: 45000\n",
            "D loss: 0.5533\n",
            "G_loss: 2.63\n",
            "\n",
            "Iter: 46000\n",
            "D loss: 0.5536\n",
            "G_loss: 2.357\n",
            "\n",
            "Iter: 47000\n",
            "D loss: 0.6887\n",
            "G_loss: 2.732\n",
            "\n",
            "Iter: 48000\n",
            "D loss: 0.4778\n",
            "G_loss: 2.853\n",
            "\n",
            "Iter: 49000\n",
            "D loss: 0.6221\n",
            "G_loss: 2.547\n",
            "\n",
            "Iter: 50000\n",
            "D loss: 0.6625\n",
            "G_loss: 2.622\n",
            "\n",
            "Iter: 51000\n",
            "D loss: 0.474\n",
            "G_loss: 2.412\n",
            "\n",
            "Iter: 52000\n",
            "D loss: 0.5779\n",
            "G_loss: 2.415\n",
            "\n",
            "Iter: 53000\n",
            "D loss: 0.5054\n",
            "G_loss: 2.555\n",
            "\n",
            "Iter: 54000\n",
            "D loss: 0.6205\n",
            "G_loss: 2.52\n",
            "\n",
            "Iter: 55000\n",
            "D loss: 0.581\n",
            "G_loss: 2.778\n",
            "\n",
            "oh no loss became nan at iteration 55304\n",
            "Iter: 0\n",
            "D loss: 1.987\n",
            "G_loss: 1.355\n",
            "\n",
            "Iter: 1000\n",
            "D loss: 0.004372\n",
            "G_loss: 9.179\n",
            "\n",
            "Iter: 2000\n",
            "D loss: 0.02924\n",
            "G_loss: 5.97\n",
            "\n",
            "Iter: 3000\n",
            "D loss: 0.07659\n",
            "G_loss: 5.465\n",
            "\n",
            "Iter: 4000\n",
            "D loss: 0.2362\n",
            "G_loss: 4.453\n",
            "\n",
            "Iter: 5000\n",
            "D loss: 0.3082\n",
            "G_loss: 4.7\n",
            "\n",
            "Iter: 6000\n",
            "D loss: 0.6358\n",
            "G_loss: 4.388\n",
            "\n",
            "Iter: 7000\n",
            "D loss: 0.3503\n",
            "G_loss: 3.313\n",
            "\n",
            "Iter: 8000\n",
            "D loss: 0.8262\n",
            "G_loss: 3.341\n",
            "\n",
            "Iter: 9000\n",
            "D loss: 0.6289\n",
            "G_loss: 3.046\n",
            "\n",
            "Iter: 10000\n",
            "D loss: 0.6418\n",
            "G_loss: 3.124\n",
            "\n",
            "Iter: 11000\n",
            "D loss: 0.5914\n",
            "G_loss: 3.06\n",
            "\n",
            "Iter: 12000\n",
            "D loss: 0.678\n",
            "G_loss: 2.669\n",
            "\n",
            "Iter: 13000\n",
            "D loss: 0.8342\n",
            "G_loss: 2.303\n",
            "\n",
            "Iter: 14000\n",
            "D loss: 0.5922\n",
            "G_loss: 2.613\n",
            "\n",
            "Iter: 15000\n",
            "D loss: 0.7575\n",
            "G_loss: 2.573\n",
            "\n",
            "Iter: 16000\n",
            "D loss: 0.7611\n",
            "G_loss: 2.462\n",
            "\n",
            "Iter: 17000\n",
            "D loss: 0.8403\n",
            "G_loss: 2.214\n",
            "\n",
            "Iter: 18000\n",
            "D loss: 0.9052\n",
            "G_loss: 2.329\n",
            "\n",
            "Iter: 19000\n",
            "D loss: 0.7665\n",
            "G_loss: 2.149\n",
            "\n",
            "Iter: 20000\n",
            "D loss: 0.8473\n",
            "G_loss: 1.769\n",
            "\n",
            "Iter: 21000\n",
            "D loss: 0.7982\n",
            "G_loss: 1.937\n",
            "\n",
            "Iter: 22000\n",
            "D loss: 0.8313\n",
            "G_loss: 2.041\n",
            "\n",
            "Iter: 23000\n",
            "D loss: 0.6216\n",
            "G_loss: 1.956\n",
            "\n",
            "Iter: 24000\n",
            "D loss: 0.8807\n",
            "G_loss: 2.21\n",
            "\n",
            "Iter: 25000\n",
            "D loss: 0.7619\n",
            "G_loss: 1.971\n",
            "\n",
            "Iter: 26000\n",
            "D loss: 0.665\n",
            "G_loss: 1.907\n",
            "\n",
            "Iter: 27000\n",
            "D loss: 0.6981\n",
            "G_loss: 2.486\n",
            "\n",
            "Iter: 28000\n",
            "D loss: 0.9908\n",
            "G_loss: 1.964\n",
            "\n",
            "Iter: 29000\n",
            "D loss: 0.7245\n",
            "G_loss: 2.046\n",
            "\n",
            "Iter: 30000\n",
            "D loss: 0.7849\n",
            "G_loss: 2.339\n",
            "\n",
            "Iter: 31000\n",
            "D loss: 0.6421\n",
            "G_loss: 2.169\n",
            "\n",
            "Iter: 32000\n",
            "D loss: 0.6382\n",
            "G_loss: 1.997\n",
            "\n",
            "Iter: 33000\n",
            "D loss: 0.6717\n",
            "G_loss: 2.308\n",
            "\n",
            "Iter: 34000\n",
            "D loss: 0.7093\n",
            "G_loss: 2.378\n",
            "\n",
            "Iter: 35000\n",
            "D loss: 0.7548\n",
            "G_loss: 2.453\n",
            "\n",
            "Iter: 36000\n",
            "D loss: 0.5996\n",
            "G_loss: 2.399\n",
            "\n",
            "Iter: 37000\n",
            "D loss: 0.6124\n",
            "G_loss: 2.52\n",
            "\n",
            "Iter: 38000\n",
            "D loss: 0.6684\n",
            "G_loss: 2.244\n",
            "\n",
            "Iter: 39000\n",
            "D loss: 0.7073\n",
            "G_loss: 2.111\n",
            "\n",
            "Iter: 40000\n",
            "D loss: 0.5163\n",
            "G_loss: 2.646\n",
            "\n",
            "Iter: 41000\n",
            "D loss: 0.7497\n",
            "G_loss: 2.222\n",
            "\n",
            "Iter: 42000\n",
            "D loss: 0.7322\n",
            "G_loss: 2.282\n",
            "\n",
            "Iter: 43000\n",
            "D loss: 0.6387\n",
            "G_loss: 2.757\n",
            "\n",
            "Iter: 44000\n",
            "D loss: 0.6932\n",
            "G_loss: 2.586\n",
            "\n",
            "Iter: 45000\n",
            "D loss: 0.6297\n",
            "G_loss: 2.733\n",
            "\n",
            "Iter: 46000\n",
            "D loss: 0.6507\n",
            "G_loss: 2.708\n",
            "\n",
            "Iter: 47000\n",
            "D loss: 0.7012\n",
            "G_loss: 2.407\n",
            "\n",
            "Iter: 48000\n",
            "D loss: 0.5696\n",
            "G_loss: 2.378\n",
            "\n",
            "Iter: 49000\n",
            "D loss: 0.697\n",
            "G_loss: 2.473\n",
            "\n",
            "Iter: 50000\n",
            "D loss: 0.7426\n",
            "G_loss: 2.22\n",
            "\n",
            "Iter: 51000\n",
            "D loss: 0.7406\n",
            "G_loss: 2.234\n",
            "\n",
            "Iter: 52000\n",
            "D loss: 0.5586\n",
            "G_loss: 2.782\n",
            "\n",
            "Iter: 53000\n",
            "D loss: 0.7771\n",
            "G_loss: 2.411\n",
            "\n",
            "Iter: 54000\n",
            "D loss: 0.7209\n",
            "G_loss: 2.517\n",
            "\n",
            "Iter: 55000\n",
            "D loss: 0.791\n",
            "G_loss: 2.226\n",
            "\n",
            "Iter: 56000\n",
            "D loss: 0.6515\n",
            "G_loss: 2.579\n",
            "\n",
            "Iter: 57000\n",
            "D loss: 0.7018\n",
            "G_loss: 2.27\n",
            "\n",
            "Iter: 58000\n",
            "D loss: 0.5189\n",
            "G_loss: 2.347\n",
            "\n",
            "Iter: 59000\n",
            "D loss: 0.7005\n",
            "G_loss: 2.119\n",
            "\n",
            "Iter: 60000\n",
            "D loss: 0.6421\n",
            "G_loss: 2.597\n",
            "\n",
            "Iter: 61000\n",
            "D loss: 0.7049\n",
            "G_loss: 2.664\n",
            "\n",
            "Iter: 62000\n",
            "D loss: 0.5977\n",
            "G_loss: 2.284\n",
            "\n",
            "Iter: 63000\n",
            "D loss: 0.6306\n",
            "G_loss: 2.147\n",
            "\n",
            "Iter: 64000\n",
            "D loss: 0.6831\n",
            "G_loss: 2.44\n",
            "\n",
            "Iter: 65000\n",
            "D loss: 0.7617\n",
            "G_loss: 2.25\n",
            "\n",
            "oh no loss became nan at iteration 65679\n",
            "Iter: 0\n",
            "D loss: 1.392\n",
            "G_loss: 2.521\n",
            "\n",
            "Iter: 1000\n",
            "D loss: 0.008799\n",
            "G_loss: 10.28\n",
            "\n",
            "Iter: 2000\n",
            "D loss: 0.04452\n",
            "G_loss: 4.795\n",
            "\n",
            "Iter: 3000\n",
            "D loss: 0.1042\n",
            "G_loss: 4.614\n",
            "\n",
            "Iter: 4000\n",
            "D loss: 0.3088\n",
            "G_loss: 5.217\n",
            "\n",
            "Iter: 5000\n",
            "D loss: 0.2246\n",
            "G_loss: 4.128\n",
            "\n",
            "Iter: 6000\n",
            "D loss: 0.2497\n",
            "G_loss: 4.591\n",
            "\n",
            "Iter: 7000\n",
            "D loss: 0.5714\n",
            "G_loss: 4.016\n",
            "\n",
            "Iter: 8000\n",
            "D loss: 0.5628\n",
            "G_loss: 3.637\n",
            "\n",
            "Iter: 9000\n",
            "D loss: 0.6638\n",
            "G_loss: 2.285\n",
            "\n",
            "Iter: 10000\n",
            "D loss: 0.5936\n",
            "G_loss: 2.649\n",
            "\n",
            "Iter: 11000\n",
            "D loss: 0.5817\n",
            "G_loss: 3.167\n",
            "\n",
            "Iter: 12000\n",
            "D loss: 0.8525\n",
            "G_loss: 2.145\n",
            "\n",
            "Iter: 13000\n",
            "D loss: 0.6319\n",
            "G_loss: 3.135\n",
            "\n",
            "Iter: 14000\n",
            "D loss: 0.7797\n",
            "G_loss: 2.66\n",
            "\n",
            "Iter: 15000\n",
            "D loss: 0.5957\n",
            "G_loss: 2.593\n",
            "\n",
            "Iter: 16000\n",
            "D loss: 0.8441\n",
            "G_loss: 2.331\n",
            "\n",
            "Iter: 17000\n",
            "D loss: 0.6273\n",
            "G_loss: 2.415\n",
            "\n",
            "Iter: 18000\n",
            "D loss: 0.7307\n",
            "G_loss: 2.475\n",
            "\n",
            "Iter: 19000\n",
            "D loss: 0.8467\n",
            "G_loss: 2.19\n",
            "\n",
            "Iter: 20000\n",
            "D loss: 0.6664\n",
            "G_loss: 2.332\n",
            "\n",
            "Iter: 21000\n",
            "D loss: 0.8621\n",
            "G_loss: 2.235\n",
            "\n",
            "Iter: 22000\n",
            "D loss: 0.9583\n",
            "G_loss: 2.061\n",
            "\n",
            "Iter: 23000\n",
            "D loss: 0.8555\n",
            "G_loss: 2.274\n",
            "\n",
            "Iter: 24000\n",
            "D loss: 0.7734\n",
            "G_loss: 1.798\n",
            "\n",
            "Iter: 25000\n",
            "D loss: 0.7619\n",
            "G_loss: 2.13\n",
            "\n",
            "Iter: 26000\n",
            "D loss: 0.7967\n",
            "G_loss: 2.021\n",
            "\n",
            "Iter: 27000\n",
            "D loss: 0.851\n",
            "G_loss: 1.997\n",
            "\n",
            "Iter: 28000\n",
            "D loss: 0.8272\n",
            "G_loss: 2.125\n",
            "\n",
            "Iter: 29000\n",
            "D loss: 0.7202\n",
            "G_loss: 2.185\n",
            "\n",
            "Iter: 30000\n",
            "D loss: 0.8181\n",
            "G_loss: 1.945\n",
            "\n",
            "Iter: 31000\n",
            "D loss: 0.7406\n",
            "G_loss: 2.003\n",
            "\n",
            "Iter: 32000\n",
            "D loss: 0.7422\n",
            "G_loss: 2.122\n",
            "\n",
            "Iter: 33000\n",
            "D loss: 0.5031\n",
            "G_loss: 2.125\n",
            "\n",
            "Iter: 34000\n",
            "D loss: 0.6551\n",
            "G_loss: 1.797\n",
            "\n",
            "Iter: 35000\n",
            "D loss: 0.7598\n",
            "G_loss: 1.75\n",
            "\n",
            "Iter: 36000\n",
            "D loss: 0.6447\n",
            "G_loss: 2.38\n",
            "\n",
            "Iter: 37000\n",
            "D loss: 0.7479\n",
            "G_loss: 2.2\n",
            "\n",
            "Iter: 38000\n",
            "D loss: 0.7419\n",
            "G_loss: 2.29\n",
            "\n",
            "Iter: 39000\n",
            "D loss: 0.6817\n",
            "G_loss: 2.27\n",
            "\n",
            "Iter: 40000\n",
            "D loss: 0.7492\n",
            "G_loss: 2.203\n",
            "\n",
            "Iter: 41000\n",
            "D loss: 0.6883\n",
            "G_loss: 2.397\n",
            "\n",
            "Iter: 42000\n",
            "D loss: 0.6967\n",
            "G_loss: 2.441\n",
            "\n",
            "Iter: 43000\n",
            "D loss: 0.6485\n",
            "G_loss: 2.477\n",
            "\n",
            "Iter: 44000\n",
            "D loss: 0.6949\n",
            "G_loss: 2.151\n",
            "\n",
            "Iter: 45000\n",
            "D loss: 0.6185\n",
            "G_loss: 2.259\n",
            "\n",
            "Iter: 46000\n",
            "D loss: 0.6936\n",
            "G_loss: 2.038\n",
            "\n",
            "Iter: 47000\n",
            "D loss: 0.7576\n",
            "G_loss: 2.123\n",
            "\n",
            "Iter: 48000\n",
            "D loss: 0.641\n",
            "G_loss: 2.551\n",
            "\n",
            "Iter: 49000\n",
            "D loss: 0.6621\n",
            "G_loss: 2.488\n",
            "\n",
            "Iter: 50000\n",
            "D loss: 0.5866\n",
            "G_loss: 2.262\n",
            "\n",
            "Iter: 51000\n",
            "D loss: 0.8417\n",
            "G_loss: 2.24\n",
            "\n",
            "Iter: 52000\n",
            "D loss: 0.5725\n",
            "G_loss: 2.399\n",
            "\n",
            "Iter: 53000\n",
            "D loss: 0.7426\n",
            "G_loss: 2.225\n",
            "\n",
            "Iter: 54000\n",
            "D loss: 0.6989\n",
            "G_loss: 2.455\n",
            "\n",
            "Iter: 55000\n",
            "D loss: 0.7358\n",
            "G_loss: 2.544\n",
            "\n",
            "Iter: 56000\n",
            "D loss: 0.6729\n",
            "G_loss: 2.238\n",
            "\n",
            "Iter: 57000\n",
            "D loss: 0.7129\n",
            "G_loss: 2.222\n",
            "\n",
            "Iter: 58000\n",
            "D loss: 0.5654\n",
            "G_loss: 2.328\n",
            "\n",
            "Iter: 59000\n",
            "D loss: 0.6873\n",
            "G_loss: 1.979\n",
            "\n",
            "Iter: 60000\n",
            "D loss: 0.5877\n",
            "G_loss: 2.368\n",
            "\n",
            "Iter: 61000\n",
            "D loss: 0.7524\n",
            "G_loss: 2.452\n",
            "\n",
            "Iter: 62000\n",
            "D loss: 0.6037\n",
            "G_loss: 2.317\n",
            "\n",
            "Iter: 63000\n",
            "D loss: 0.6747\n",
            "G_loss: 2.358\n",
            "\n",
            "Iter: 64000\n",
            "D loss: 0.6157\n",
            "G_loss: 2.116\n",
            "\n",
            "Iter: 65000\n",
            "D loss: 0.726\n",
            "G_loss: 1.988\n",
            "\n",
            "Iter: 66000\n",
            "D loss: 0.6511\n",
            "G_loss: 2.108\n",
            "\n",
            "Iter: 67000\n",
            "D loss: 0.6213\n",
            "G_loss: 2.317\n",
            "\n",
            "Iter: 68000\n",
            "D loss: 0.6489\n",
            "G_loss: 2.246\n",
            "\n",
            "Iter: 69000\n",
            "D loss: 0.591\n",
            "G_loss: 2.616\n",
            "\n",
            "Iter: 70000\n",
            "D loss: 0.5644\n",
            "G_loss: 2.626\n",
            "\n",
            "Iter: 71000\n",
            "D loss: 0.607\n",
            "G_loss: 1.993\n",
            "\n",
            "Iter: 72000\n",
            "D loss: 0.5854\n",
            "G_loss: 2.21\n",
            "\n",
            "Iter: 73000\n",
            "D loss: 0.5051\n",
            "G_loss: 2.333\n",
            "\n",
            "Iter: 74000\n",
            "D loss: 0.6382\n",
            "G_loss: 2.398\n",
            "\n",
            "Iter: 75000\n",
            "D loss: 0.5744\n",
            "G_loss: 2.145\n",
            "\n",
            "Iter: 76000\n",
            "D loss: 0.6925\n",
            "G_loss: 2.441\n",
            "\n",
            "Iter: 77000\n",
            "D loss: 0.5949\n",
            "G_loss: 2.33\n",
            "\n",
            "Iter: 78000\n",
            "D loss: 0.6096\n",
            "G_loss: 2.102\n",
            "\n",
            "Iter: 79000\n",
            "D loss: 0.6533\n",
            "G_loss: 2.249\n",
            "\n",
            "Iter: 80000\n",
            "D loss: 0.7097\n",
            "G_loss: 2.27\n",
            "\n",
            "Iter: 81000\n",
            "D loss: 0.695\n",
            "G_loss: 2.409\n",
            "\n",
            "Iter: 82000\n",
            "D loss: 0.5661\n",
            "G_loss: 2.35\n",
            "\n",
            "Iter: 83000\n",
            "D loss: 0.5673\n",
            "G_loss: 2.407\n",
            "\n",
            "Iter: 84000\n",
            "D loss: 0.6356\n",
            "G_loss: 2.481\n",
            "\n",
            "Iter: 85000\n",
            "D loss: 0.6036\n",
            "G_loss: 2.416\n",
            "\n",
            "Iter: 86000\n",
            "D loss: 0.6842\n",
            "G_loss: 2.139\n",
            "\n",
            "Iter: 87000\n",
            "D loss: 0.5563\n",
            "G_loss: 2.373\n",
            "\n",
            "Iter: 88000\n",
            "D loss: 0.5491\n",
            "G_loss: 2.516\n",
            "\n",
            "Iter: 89000\n",
            "D loss: 0.6309\n",
            "G_loss: 2.48\n",
            "\n",
            "Iter: 90000\n",
            "D loss: 0.506\n",
            "G_loss: 2.177\n",
            "\n",
            "oh no loss became nan at iteration 90479\n",
            "Iter: 0\n",
            "D loss: 1.546\n",
            "G_loss: 2.162\n",
            "\n",
            "Iter: 1000\n",
            "D loss: 0.005244\n",
            "G_loss: 8.121\n",
            "\n",
            "Iter: 2000\n",
            "D loss: 0.06787\n",
            "G_loss: 6.522\n",
            "\n",
            "Iter: 3000\n",
            "D loss: 0.06605\n",
            "G_loss: 5.045\n",
            "\n",
            "Iter: 4000\n",
            "D loss: 0.1188\n",
            "G_loss: 5.313\n",
            "\n",
            "Iter: 5000\n",
            "D loss: 0.2229\n",
            "G_loss: 5.057\n",
            "\n",
            "Iter: 6000\n",
            "D loss: 0.4872\n",
            "G_loss: 3.406\n",
            "\n",
            "Iter: 7000\n",
            "D loss: 0.3826\n",
            "G_loss: 3.561\n",
            "\n",
            "Iter: 8000\n",
            "D loss: 0.2257\n",
            "G_loss: 3.979\n",
            "\n",
            "Iter: 9000\n",
            "D loss: 0.3613\n",
            "G_loss: 3.284\n",
            "\n",
            "Iter: 10000\n",
            "D loss: 0.6162\n",
            "G_loss: 2.965\n",
            "\n",
            "Iter: 11000\n",
            "D loss: 0.428\n",
            "G_loss: 2.467\n",
            "\n",
            "Iter: 12000\n",
            "D loss: 0.4588\n",
            "G_loss: 3.517\n",
            "\n",
            "Iter: 13000\n",
            "D loss: 0.3929\n",
            "G_loss: 2.493\n",
            "\n",
            "Iter: 14000\n",
            "D loss: 0.6165\n",
            "G_loss: 2.698\n",
            "\n",
            "Iter: 15000\n",
            "D loss: 0.5365\n",
            "G_loss: 2.546\n",
            "\n",
            "Iter: 16000\n",
            "D loss: 0.7316\n",
            "G_loss: 2.396\n",
            "\n",
            "Iter: 17000\n",
            "D loss: 0.5972\n",
            "G_loss: 2.669\n",
            "\n",
            "Iter: 18000\n",
            "D loss: 0.5344\n",
            "G_loss: 2.221\n",
            "\n",
            "Iter: 19000\n",
            "D loss: 0.6214\n",
            "G_loss: 2.406\n",
            "\n",
            "Iter: 20000\n",
            "D loss: 0.5395\n",
            "G_loss: 2.266\n",
            "\n",
            "Iter: 21000\n",
            "D loss: 0.532\n",
            "G_loss: 2.674\n",
            "\n",
            "Iter: 22000\n",
            "D loss: 0.6888\n",
            "G_loss: 2.471\n",
            "\n",
            "Iter: 23000\n",
            "D loss: 0.7068\n",
            "G_loss: 2.426\n",
            "\n",
            "Iter: 24000\n",
            "D loss: 0.5692\n",
            "G_loss: 2.366\n",
            "\n",
            "Iter: 25000\n",
            "D loss: 0.7576\n",
            "G_loss: 2.491\n",
            "\n",
            "Iter: 26000\n",
            "D loss: 0.6128\n",
            "G_loss: 2.448\n",
            "\n",
            "Iter: 27000\n",
            "D loss: 0.7351\n",
            "G_loss: 2.223\n",
            "\n",
            "Iter: 28000\n",
            "D loss: 0.5643\n",
            "G_loss: 2.507\n",
            "\n",
            "Iter: 29000\n",
            "D loss: 0.5995\n",
            "G_loss: 2.226\n",
            "\n",
            "Iter: 30000\n",
            "D loss: 0.6336\n",
            "G_loss: 2.417\n",
            "\n",
            "Iter: 31000\n",
            "D loss: 0.6782\n",
            "G_loss: 2.318\n",
            "\n",
            "Iter: 32000\n",
            "D loss: 0.5887\n",
            "G_loss: 2.57\n",
            "\n",
            "Iter: 33000\n",
            "D loss: 0.5618\n",
            "G_loss: 2.412\n",
            "\n",
            "Iter: 34000\n",
            "D loss: 0.6179\n",
            "G_loss: 2.381\n",
            "\n",
            "Iter: 35000\n",
            "D loss: 0.4883\n",
            "G_loss: 2.329\n",
            "\n",
            "Iter: 36000\n",
            "D loss: 0.5942\n",
            "G_loss: 2.167\n",
            "\n",
            "Iter: 37000\n",
            "D loss: 0.6094\n",
            "G_loss: 2.054\n",
            "\n",
            "Iter: 38000\n",
            "D loss: 0.6961\n",
            "G_loss: 2.377\n",
            "\n",
            "Iter: 39000\n",
            "D loss: 0.5765\n",
            "G_loss: 2.501\n",
            "\n",
            "Iter: 40000\n",
            "D loss: 0.5367\n",
            "G_loss: 2.543\n",
            "\n",
            "Iter: 41000\n",
            "D loss: 0.5765\n",
            "G_loss: 2.43\n",
            "\n",
            "Iter: 42000\n",
            "D loss: 0.7007\n",
            "G_loss: 2.422\n",
            "\n",
            "Iter: 43000\n",
            "D loss: 0.6081\n",
            "G_loss: 2.155\n",
            "\n",
            "Iter: 44000\n",
            "D loss: 0.4406\n",
            "G_loss: 2.397\n",
            "\n",
            "Iter: 45000\n",
            "D loss: 0.5161\n",
            "G_loss: 2.491\n",
            "\n",
            "Iter: 46000\n",
            "D loss: 0.5905\n",
            "G_loss: 2.396\n",
            "\n",
            "Iter: 47000\n",
            "D loss: 0.5152\n",
            "G_loss: 2.369\n",
            "\n",
            "Iter: 48000\n",
            "D loss: 0.6188\n",
            "G_loss: 2.199\n",
            "\n",
            "Iter: 49000\n",
            "D loss: 0.5735\n",
            "G_loss: 2.7\n",
            "\n",
            "Iter: 50000\n",
            "D loss: 0.6434\n",
            "G_loss: 2.445\n",
            "\n",
            "Iter: 51000\n",
            "D loss: 0.4781\n",
            "G_loss: 2.434\n",
            "\n",
            "Iter: 52000\n",
            "D loss: 0.5242\n",
            "G_loss: 2.403\n",
            "\n",
            "Iter: 53000\n",
            "D loss: 0.6298\n",
            "G_loss: 2.692\n",
            "\n",
            "Iter: 54000\n",
            "D loss: 0.5501\n",
            "G_loss: 2.437\n",
            "\n",
            "Iter: 55000\n",
            "D loss: 0.5573\n",
            "G_loss: 2.471\n",
            "\n",
            "Iter: 56000\n",
            "D loss: 0.5513\n",
            "G_loss: 2.67\n",
            "\n",
            "Iter: 57000\n",
            "D loss: 0.5727\n",
            "G_loss: 2.642\n",
            "\n",
            "Iter: 58000\n",
            "D loss: 0.6487\n",
            "G_loss: 2.308\n",
            "\n",
            "Iter: 59000\n",
            "D loss: 0.6516\n",
            "G_loss: 2.193\n",
            "\n",
            "Iter: 60000\n",
            "D loss: 0.5092\n",
            "G_loss: 2.439\n",
            "\n",
            "Iter: 61000\n",
            "D loss: 0.4604\n",
            "G_loss: 2.782\n",
            "\n",
            "Iter: 62000\n",
            "D loss: 0.5057\n",
            "G_loss: 2.461\n",
            "\n",
            "Iter: 63000\n",
            "D loss: 0.4529\n",
            "G_loss: 2.096\n",
            "\n",
            "Iter: 64000\n",
            "D loss: 0.4234\n",
            "G_loss: 2.666\n",
            "\n",
            "Iter: 65000\n",
            "D loss: 0.6809\n",
            "G_loss: 2.591\n",
            "\n",
            "Iter: 66000\n",
            "D loss: 0.479\n",
            "G_loss: 2.2\n",
            "\n",
            "Iter: 67000\n",
            "D loss: 0.5094\n",
            "G_loss: 2.316\n",
            "\n",
            "Iter: 68000\n",
            "D loss: 0.6291\n",
            "G_loss: 2.396\n",
            "\n",
            "Iter: 69000\n",
            "D loss: 0.5443\n",
            "G_loss: 2.503\n",
            "\n",
            "Iter: 70000\n",
            "D loss: 0.4484\n",
            "G_loss: 2.461\n",
            "\n",
            "Iter: 71000\n",
            "D loss: 0.4568\n",
            "G_loss: 2.704\n",
            "\n",
            "Iter: 72000\n",
            "D loss: 0.4998\n",
            "G_loss: 2.244\n",
            "\n",
            "Iter: 73000\n",
            "D loss: 0.5207\n",
            "G_loss: 2.81\n",
            "\n",
            "Iter: 74000\n",
            "D loss: 0.5004\n",
            "G_loss: 2.378\n",
            "\n",
            "Iter: 75000\n",
            "D loss: 0.491\n",
            "G_loss: 2.612\n",
            "\n",
            "Iter: 76000\n",
            "D loss: 0.5668\n",
            "G_loss: 2.424\n",
            "\n",
            "Iter: 77000\n",
            "D loss: 0.4486\n",
            "G_loss: 2.344\n",
            "\n",
            "Iter: 78000\n",
            "D loss: 0.5755\n",
            "G_loss: 2.024\n",
            "\n",
            "Iter: 79000\n",
            "D loss: 0.5004\n",
            "G_loss: 2.341\n",
            "\n",
            "Iter: 80000\n",
            "D loss: 0.4459\n",
            "G_loss: 2.605\n",
            "\n",
            "Iter: 81000\n",
            "D loss: 0.3746\n",
            "G_loss: 2.628\n",
            "\n",
            "Iter: 82000\n",
            "D loss: 0.4597\n",
            "G_loss: 2.967\n",
            "\n",
            "Iter: 83000\n",
            "D loss: 0.452\n",
            "G_loss: 2.238\n",
            "\n",
            "oh no loss became nan at iteration 83731\n",
            "Iter: 0\n",
            "D loss: 1.321\n",
            "G_loss: 2.527\n",
            "\n",
            "Iter: 1000\n",
            "D loss: 0.003065\n",
            "G_loss: 9.0\n",
            "\n",
            "Iter: 2000\n",
            "D loss: 0.06777\n",
            "G_loss: 4.602\n",
            "\n",
            "Iter: 3000\n",
            "D loss: 0.07352\n",
            "G_loss: 4.533\n",
            "\n",
            "Iter: 4000\n",
            "D loss: 0.1735\n",
            "G_loss: 5.07\n",
            "\n",
            "Iter: 5000\n",
            "D loss: 0.4226\n",
            "G_loss: 4.517\n",
            "\n",
            "Iter: 6000\n",
            "D loss: 0.2748\n",
            "G_loss: 4.472\n",
            "\n",
            "Iter: 7000\n",
            "D loss: 0.3869\n",
            "G_loss: 3.716\n",
            "\n",
            "Iter: 8000\n",
            "D loss: 0.598\n",
            "G_loss: 4.303\n",
            "\n",
            "Iter: 9000\n",
            "D loss: 0.5473\n",
            "G_loss: 2.83\n",
            "\n",
            "Iter: 10000\n",
            "D loss: 0.5965\n",
            "G_loss: 2.55\n",
            "\n",
            "Iter: 11000\n",
            "D loss: 0.7827\n",
            "G_loss: 3.054\n",
            "\n",
            "Iter: 12000\n",
            "D loss: 0.7683\n",
            "G_loss: 2.699\n",
            "\n",
            "Iter: 13000\n",
            "D loss: 0.5485\n",
            "G_loss: 2.806\n",
            "\n",
            "Iter: 14000\n",
            "D loss: 0.6008\n",
            "G_loss: 2.617\n",
            "\n",
            "Iter: 15000\n",
            "D loss: 0.7439\n",
            "G_loss: 2.436\n",
            "\n",
            "Iter: 16000\n",
            "D loss: 0.8068\n",
            "G_loss: 2.202\n",
            "\n",
            "Iter: 17000\n",
            "D loss: 0.6354\n",
            "G_loss: 2.482\n",
            "\n",
            "Iter: 18000\n",
            "D loss: 0.7534\n",
            "G_loss: 2.092\n",
            "\n",
            "Iter: 19000\n",
            "D loss: 0.7323\n",
            "G_loss: 2.159\n",
            "\n",
            "Iter: 20000\n",
            "D loss: 0.814\n",
            "G_loss: 2.013\n",
            "\n",
            "Iter: 21000\n",
            "D loss: 0.7704\n",
            "G_loss: 2.349\n",
            "\n",
            "Iter: 22000\n",
            "D loss: 0.7668\n",
            "G_loss: 2.161\n",
            "\n",
            "Iter: 23000\n",
            "D loss: 0.6855\n",
            "G_loss: 2.573\n",
            "\n",
            "Iter: 24000\n",
            "D loss: 0.726\n",
            "G_loss: 1.918\n",
            "\n",
            "Iter: 25000\n",
            "D loss: 0.6681\n",
            "G_loss: 1.944\n",
            "\n",
            "Iter: 26000\n",
            "D loss: 0.6249\n",
            "G_loss: 2.478\n",
            "\n",
            "Iter: 27000\n",
            "D loss: 0.6831\n",
            "G_loss: 2.01\n",
            "\n",
            "Iter: 28000\n",
            "D loss: 0.7333\n",
            "G_loss: 1.836\n",
            "\n",
            "Iter: 29000\n",
            "D loss: 0.7011\n",
            "G_loss: 2.113\n",
            "\n",
            "Iter: 30000\n",
            "D loss: 0.6575\n",
            "G_loss: 2.373\n",
            "\n",
            "Iter: 31000\n",
            "D loss: 0.5995\n",
            "G_loss: 2.216\n",
            "\n",
            "Iter: 32000\n",
            "D loss: 0.7084\n",
            "G_loss: 2.117\n",
            "\n",
            "Iter: 33000\n",
            "D loss: 0.6878\n",
            "G_loss: 2.461\n",
            "\n",
            "Iter: 34000\n",
            "D loss: 0.8142\n",
            "G_loss: 2.222\n",
            "\n",
            "Iter: 35000\n",
            "D loss: 0.6327\n",
            "G_loss: 2.206\n",
            "\n",
            "Iter: 36000\n",
            "D loss: 0.7851\n",
            "G_loss: 2.073\n",
            "\n",
            "Iter: 37000\n",
            "D loss: 0.6913\n",
            "G_loss: 2.331\n",
            "\n",
            "Iter: 38000\n",
            "D loss: 0.7373\n",
            "G_loss: 1.87\n",
            "\n",
            "Iter: 39000\n",
            "D loss: 0.748\n",
            "G_loss: 2.338\n",
            "\n",
            "Iter: 40000\n",
            "D loss: 0.693\n",
            "G_loss: 2.24\n",
            "\n",
            "Iter: 41000\n",
            "D loss: 0.7716\n",
            "G_loss: 2.03\n",
            "\n",
            "Iter: 42000\n",
            "D loss: 0.6273\n",
            "G_loss: 2.177\n",
            "\n",
            "Iter: 43000\n",
            "D loss: 0.7563\n",
            "G_loss: 2.031\n",
            "\n",
            "Iter: 44000\n",
            "D loss: 0.5265\n",
            "G_loss: 2.573\n",
            "\n",
            "Iter: 45000\n",
            "D loss: 0.6639\n",
            "G_loss: 2.241\n",
            "\n",
            "Iter: 46000\n",
            "D loss: 0.7685\n",
            "G_loss: 2.366\n",
            "\n",
            "Iter: 47000\n",
            "D loss: 0.63\n",
            "G_loss: 2.617\n",
            "\n",
            "Iter: 48000\n",
            "D loss: 0.7438\n",
            "G_loss: 2.064\n",
            "\n",
            "Iter: 49000\n",
            "D loss: 0.489\n",
            "G_loss: 2.509\n",
            "\n",
            "Iter: 50000\n",
            "D loss: 0.7924\n",
            "G_loss: 2.274\n",
            "\n",
            "Iter: 51000\n",
            "D loss: 0.6538\n",
            "G_loss: 2.217\n",
            "\n",
            "Iter: 52000\n",
            "D loss: 0.5071\n",
            "G_loss: 2.247\n",
            "\n",
            "Iter: 53000\n",
            "D loss: 0.6414\n",
            "G_loss: 2.285\n",
            "\n",
            "Iter: 54000\n",
            "D loss: 0.6964\n",
            "G_loss: 2.059\n",
            "\n",
            "Iter: 55000\n",
            "D loss: 0.6655\n",
            "G_loss: 2.135\n",
            "\n",
            "Iter: 56000\n",
            "D loss: 0.6697\n",
            "G_loss: 2.178\n",
            "\n",
            "Iter: 57000\n",
            "D loss: 0.563\n",
            "G_loss: 2.202\n",
            "\n",
            "Iter: 58000\n",
            "D loss: 0.6146\n",
            "G_loss: 2.042\n",
            "\n",
            "Iter: 59000\n",
            "D loss: 0.6593\n",
            "G_loss: 2.328\n",
            "\n",
            "Iter: 60000\n",
            "D loss: 0.7934\n",
            "G_loss: 2.383\n",
            "\n",
            "Iter: 61000\n",
            "D loss: 0.5688\n",
            "G_loss: 2.2\n",
            "\n",
            "Iter: 62000\n",
            "D loss: 0.5471\n",
            "G_loss: 2.51\n",
            "\n",
            "Iter: 63000\n",
            "D loss: 0.6796\n",
            "G_loss: 2.143\n",
            "\n",
            "Iter: 64000\n",
            "D loss: 0.5983\n",
            "G_loss: 2.193\n",
            "\n",
            "Iter: 65000\n",
            "D loss: 0.6188\n",
            "G_loss: 2.189\n",
            "\n",
            "Iter: 66000\n",
            "D loss: 0.6222\n",
            "G_loss: 2.246\n",
            "\n",
            "Iter: 67000\n",
            "D loss: 0.5451\n",
            "G_loss: 2.373\n",
            "\n",
            "Iter: 68000\n",
            "D loss: 0.7203\n",
            "G_loss: 2.474\n",
            "\n",
            "Iter: 69000\n",
            "D loss: 0.6995\n",
            "G_loss: 2.5\n",
            "\n",
            "Iter: 70000\n",
            "D loss: 0.5463\n",
            "G_loss: 2.619\n",
            "\n",
            "Iter: 71000\n",
            "D loss: 0.5863\n",
            "G_loss: 2.196\n",
            "\n",
            "Iter: 72000\n",
            "D loss: 0.7455\n",
            "G_loss: 1.963\n",
            "\n",
            "Iter: 73000\n",
            "D loss: 0.6624\n",
            "G_loss: 2.072\n",
            "\n",
            "Iter: 74000\n",
            "D loss: 0.5925\n",
            "G_loss: 2.24\n",
            "\n",
            "Iter: 75000\n",
            "D loss: 0.5069\n",
            "G_loss: 2.311\n",
            "\n",
            "Iter: 76000\n",
            "D loss: 0.6856\n",
            "G_loss: 2.34\n",
            "\n",
            "Iter: 77000\n",
            "D loss: 0.6018\n",
            "G_loss: 2.481\n",
            "\n",
            "Iter: 78000\n",
            "D loss: 0.657\n",
            "G_loss: 2.441\n",
            "\n",
            "Iter: 79000\n",
            "D loss: 0.5604\n",
            "G_loss: 2.501\n",
            "\n",
            "Iter: 80000\n",
            "D loss: 0.5286\n",
            "G_loss: 2.148\n",
            "\n",
            "Iter: 81000\n",
            "D loss: 0.5475\n",
            "G_loss: 2.325\n",
            "\n",
            "Iter: 82000\n",
            "D loss: 0.5389\n",
            "G_loss: 2.475\n",
            "\n",
            "Iter: 83000\n",
            "D loss: 0.6903\n",
            "G_loss: 2.523\n",
            "\n",
            "Iter: 84000\n",
            "D loss: 0.4856\n",
            "G_loss: 1.936\n",
            "\n",
            "Iter: 85000\n",
            "D loss: 0.6011\n",
            "G_loss: 2.496\n",
            "\n",
            "Iter: 86000\n",
            "D loss: 0.6333\n",
            "G_loss: 2.352\n",
            "\n",
            "Iter: 87000\n",
            "D loss: 0.5952\n",
            "G_loss: 2.24\n",
            "\n",
            "Iter: 88000\n",
            "D loss: 0.606\n",
            "G_loss: 2.378\n",
            "\n",
            "Iter: 89000\n",
            "D loss: 0.6325\n",
            "G_loss: 2.308\n",
            "\n",
            "Iter: 90000\n",
            "D loss: 0.6577\n",
            "G_loss: 2.317\n",
            "\n",
            "Iter: 91000\n",
            "D loss: 0.6477\n",
            "G_loss: 2.78\n",
            "\n",
            "Iter: 92000\n",
            "D loss: 0.5548\n",
            "G_loss: 2.802\n",
            "\n",
            "Iter: 93000\n",
            "D loss: 0.5303\n",
            "G_loss: 2.778\n",
            "\n",
            "Iter: 94000\n",
            "D loss: 0.5339\n",
            "G_loss: 2.456\n",
            "\n",
            "Iter: 95000\n",
            "D loss: 0.4897\n",
            "G_loss: 2.475\n",
            "\n",
            "Iter: 96000\n",
            "D loss: 0.5013\n",
            "G_loss: 2.344\n",
            "\n",
            "Iter: 97000\n",
            "D loss: 0.5786\n",
            "G_loss: 2.559\n",
            "\n",
            "Iter: 98000\n",
            "D loss: 0.5633\n",
            "G_loss: 2.168\n",
            "\n",
            "Iter: 99000\n",
            "D loss: 0.5386\n",
            "G_loss: 2.324\n",
            "\n",
            "Iter: 100000\n",
            "D loss: 0.5214\n",
            "G_loss: 2.199\n",
            "\n",
            "7\n"
          ]
        }
      ],
      "source": [
        "attemps = 0\n",
        "while not train():\n",
        "    attemps+=1\n",
        "\n",
        "print (attemps)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}